{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afd79b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import fold, unfold\n",
    "import glob\n",
    "import _pickle as cPickle\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c0d0450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92ac8e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/scratch/venia/playground/yep/EE559-MiniProject/Proj_341752_337188_250222/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "075e4c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/scratch/venia/playground',\n",
       " '/home/veselovs/miniconda3/bin/python',\n",
       " '/home/veselovs/miniconda3/lib/python39.zip',\n",
       " '/home/veselovs/miniconda3/lib/python3.9',\n",
       " '/home/veselovs/miniconda3/lib/python3.9/lib-dynload',\n",
       " '',\n",
       " '/home/veselovs/miniconda3/lib/python3.9/site-packages',\n",
       " '/scratch/venia/playground/yep/EE559-MiniProject/Proj_341752_337188_250222/data']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "2384079e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "f71751d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n1, n2 = torch.load(\"train_data.pkl\",map_location=torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "51bdc31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 3 \n",
    "out_channels = 4 \n",
    "kernel_size = (2, 3)\n",
    "conv = torch.nn.Conv2d(in_channels,out_channels ,kernel_size)\n",
    "x = torch.randn((1, in_channels , 32, 32))\n",
    "# Output of PyTorch convolution\n",
    "expected = conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4a1b64c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of convolution as a matrix product\n",
    "unfolded = torch.nn.functional.unfold(x, kernel_size=kernel_size)\n",
    "wxb = conv.weight.view(out_channels, -1) @ unfolded + conv.bias.view(1, -1, 1)\n",
    "actual = wxb.view(1, out_channels, x.shape[2] - kernel_size[0] + 1, x.shape[3] - kernel_size[1]+ 1)\n",
    "torch.testing.assert_allclose(actual , expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a768390d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2803368b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1146842e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "412dcf67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1274,
   "id": "6fdeca3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.output_shape (30, 30, 30)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "in_channel = 10\n",
    "out_channel = 30\n",
    "kernel_size = (5,5)\n",
    "stride = 2\n",
    "in_size = (8,8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = torch.randn(20,in_channel, 32, 32,device = device)\n",
    "\n",
    "conv = Conv2d(out_channel,in_channel, 3, stride = 1, padding = 0)\n",
    "#og = conv.forward(x)\n",
    "#conv = Conv2d(3, 3, 3, stride = 1)\n",
    "\n",
    "fwd1 = conv.forward(x)\n",
    "conv2.weight = conv.weight\n",
    "conv2.bias = conv.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1275,
   "id": "bb72ae30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding(6,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1276,
   "id": "53c3abf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_gradient.size() torch.Size([20, 30, 30, 30])\n",
      "0\n",
      "0\n",
      "input.size() torch.Size([20, 10, 32, 32])\n",
      "zeros.size() torch.Size([20, 10, 32, 32])\n",
      "output_gradient.view(self.out_channel,-1).size() torch.Size([30, 18000])\n",
      "unfolded.size() torch.Size([10, 18000, 9])\n",
      "wxb.size() torch.Size([10, 30, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 10, 32, 32])"
      ]
     },
     "execution_count": 1276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.backward(fwd1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "id": "20bee6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(input_size, ks, stride):\n",
    "    j = None\n",
    "    for i in range(10):\n",
    "        k = (input_size - ks + i) / stride\n",
    "        if k.is_integer() == True:\n",
    "            j = i\n",
    "            break\n",
    "    return j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61001778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b70ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1115,
   "id": "28f18621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(input_size, ks, stride):\n",
    "    j = None\n",
    "    for i in range(10):\n",
    "        k = (input_size - ks + i) / stride\n",
    "        if k.is_integer() == True:\n",
    "            j = i\n",
    "            break\n",
    "    return j\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50da9fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1311,
   "id": "51714809",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(20,in_channel, 46, 46,device = device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1312,
   "id": "6f2bf85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conv.backward(og)\n",
    "conv = Conv2d(out_channel,in_channel, 3, stride = 1, padding = 0)\n",
    "upsample = Upsampling(out_channel, in_channel, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1313,
   "id": "c334be21",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsample = Upsampling(out_channel, in_channel, 3)\n",
    "\n",
    "nnupsample = NNUpsample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1314,
   "id": "ad79e4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.output_shape (30, 90, 90)\n",
      "output_gradient.size() torch.Size([20, 30, 90, 90])\n",
      "0\n",
      "0\n",
      "input.size() torch.Size([20, 10, 92, 92])\n",
      "zeros.size() torch.Size([20, 10, 92, 92])\n",
      "output_gradient.view(self.out_channel,-1).size() torch.Size([30, 162000])\n",
      "unfolded.size() torch.Size([10, 162000, 9])\n",
      "wxb.size() torch.Size([10, 30, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 10, 46, 46])"
      ]
     },
     "execution_count": 1314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnupsample.backward(conv.backward(conv.forward(nnupsample.forward(x)))).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6fae28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1315,
   "id": "4f83e668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4686ba6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a402bec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "8b5d21fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW OUTPUT SHAPE (30, 9, 9)\n",
      "NEW input SHAPE torch.Size([10, 10, 20, 20])\n",
      "self.output_shape (30, 9, 9)\n",
      "input2.size() torch.Size([10, 10, 19, 19])\n",
      "UNFOLDED.SIZE() torch.Size([10, 810, 9])\n",
      "output_gradient.size() torch.Size([10, 30, 9, 9])\n",
      "wxb.size() torch.Size([10, 30, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10, 20, 20])"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.backward(og).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a887039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "bfeb8902",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = nn.Conv2d(in_channels = in_channel,out_channels= out_channel, kernel_size = (3,3), stride = 2)\n",
    "b.weight.data = conv.weight\n",
    "b.bias.data = conv.bias\n",
    "r = b.forward(x.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0057c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d5bee94",
   "metadata": {},
   "source": [
    "input = _ _ _ _ _\n",
    "ks = 3\n",
    "stride = 2\n",
    "\n",
    "unstride_input = _0_0_0_0_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37f3d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "id": "8fbfe81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(input_size, ks, stride):\n",
    "    if stride == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        quotient = math.ceil(input_size / stride)\n",
    "        a =   (((input_size - stride+1) % ks)) % stride\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "id": "c4ac8fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 876,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding(5,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1332,
   "id": "101eac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import fold, unfold\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "class Module:\n",
    "\n",
    "\tdef forward(self, *input):\n",
    "\t\traise NotImplementedError\n",
    "\n",
    "\tdef backward(self, *gradwrtoutput):\n",
    "\t\traise NotImplementedError\n",
    "\n",
    "\tdef param(self):\n",
    "\t\treturn []\n",
    "\n",
    "class Sequential(Module):\n",
    "\tdef __init__(self, *args):\n",
    "\t\tsuper(Sequential, self).__init__()\n",
    "\n",
    "\t\tself.modules = []\n",
    "\n",
    "\t\tfor transf in args:\n",
    "\t\t\tself.modules.append(transf)\n",
    "\n",
    "\tdef __call__(self, x):\n",
    "\t\treturn self.forward(x)\n",
    "\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tfor transf in self.modules:\n",
    "\t\t\tx = transf.forward(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "\tdef state_dict(self):\n",
    "\t\tstate = list()\n",
    "\t\tfor layer in self.modules:\n",
    "\t\t\tif isinstance(layer, Conv2d):\n",
    "\t\t\t\tstate.append({'weight': layer.weight, 'bias': layer.bias})\n",
    "\t\t\telif isinstance(layer, Upsampling):\n",
    "\t\t\t\tstate.append({'weight': layer.conv.weight, 'bias': layer.conv.bias})\n",
    "\t\t\telse:\n",
    "\t\t\t\tstate.append({})\n",
    "\t\treturn state\n",
    "\n",
    "\tdef load_pretrained_model(self):\n",
    "        ## This loads the parameters saved in bestmodel.pth into the model\n",
    "\t\tmodel_path = Path(__file__).parent / \"bestmodel.pth\"\n",
    "\t\tstate_dict = torch.load(model_path)\n",
    "\n",
    "\t\tfor i, layer in enumerate(self.modules):\n",
    "\t\t\tif isinstance(layer, Conv2d):\n",
    "\t\t\t\tlayer.weight.data = state_dict[i]['weight']\n",
    "\t\t\t\tlayer.bias.data = state_dict[i]['bias']\n",
    "\t\t\telif isinstance(layer, Upsampling):\n",
    "\t\t\t\tlayer.conv.weight.data = state_dict[i]['weight']\n",
    "\t\t\t\tlayer.conv.bias.data = state_dict[i]['bias']\n",
    "\n",
    "\n",
    "\tdef backward(self,x):\n",
    "\t\tfor layer in self.modules[::-1]:\n",
    "\t\t\tx = layer.backward(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "\tdef zero_grad(self):\n",
    "\t\tfor layer in self.modules:\n",
    "\t\t\tif len(layer.param()):\n",
    "\t\t\t\tlayer.zero_grad()\n",
    "\n",
    "\n",
    "class ReLU(Module):\n",
    "\t\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(ReLU).__init__()\n",
    "\n",
    "\tdef __call__(self,x):\n",
    "\t\treturn self.forward(x)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn (x > 0).float() * x\n",
    "\n",
    "\tdef _grad(self, x):\n",
    "\t\treturn (x > 0).float()\n",
    "\n",
    "\tdef backward(self, gradwrtoutput):\n",
    "\t\treturn self._grad(gradwrtoutput)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MSE(Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MSE, self).__init__()\n",
    "        self.loss = None\n",
    "\n",
    "    def forward(self,y,y_output):\n",
    "        N = y.size(0)\n",
    "        l = 1/N*(y-y_output)**2\n",
    "        self.loss = l.sum()\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self,y,y_output):\n",
    "        # simply gradient of MSE as a function of y_output\n",
    "        N = y.size(0)\n",
    "        return -2/N*(y-y_output)\n",
    "\n",
    "import torch\n",
    "from torch.nn.functional import fold, unfold\n",
    "\n",
    "    \n",
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.model = Sequential(\n",
    "            Conv2d(48, 3, 2, 2),\n",
    "            ReLU(),\n",
    "            Conv2d(96, 48, 2, 2),\n",
    "            ReLU(),\n",
    "            Upsampling(48, 96, 3, 1, 2),\n",
    "            ReLU(),\n",
    "            Upsampling(3, 48, 3, 1, 2),\n",
    "            Sigmoid()\n",
    "        )\n",
    "    \n",
    "\n",
    "    def load_pretrained_model(self):\n",
    "        self.model.load_pretrained_model()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def predict(self, test_input):\n",
    "        test_input = test_input / 255.0\n",
    "        output = self.forward(test_input)\n",
    "        output = output * 255.0\n",
    "        return torch.clip(output, 0.0, 255.0)\n",
    "\n",
    "    def backward(self, x):\n",
    "        return self.model.backward(x)\n",
    "\n",
    "import math\n",
    "class Upsampling(Module):\n",
    "    def __init__(self, out_channel, in_channel, kernel_size, stride=1, scale_factor=2):\n",
    "        super(Upsampling).__init__()\n",
    "        self.nn = NNUpsample(scale_factor)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.conv = Conv2d(out_channel, in_channel, kernel_size, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pad = self.padding2(x.size(2), self.kernel_size, self.stride)\n",
    "        self.padding = Padding(pad)\n",
    "        x = self.nn.forward(x)\n",
    "        #x = self.padding.forward(x)\n",
    "        return self.conv.forward(x)\n",
    "\n",
    "    def backward(self, gradwrtoutput):\n",
    "        x = self.conv.backward(gradwrtoutput)\n",
    "        #x = self.padding.backward(x)\n",
    "        return self.nn.backward(x)\n",
    "    def padding2(self, input_size, ks, stride):\n",
    "        j = None\n",
    "        for i in range(10):\n",
    "            k = (input_size - ks + i) / stride\n",
    "            if k.is_integer() == True:\n",
    "                j = i\n",
    "                break\n",
    "        return j\n",
    "    \n",
    "class Padding(Module):\n",
    "    def __init__(self, padding):\n",
    "        super(Padding).__init__()\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, x):\n",
    "        padded = torch.zeros((x.size(0), x.size(1), x.size(2) + 2 * self.padding, x.size(3) + 2 * self.padding))\n",
    "        padded[:, :, self.padding:x.size(2) + self.padding, self.padding:x.size(3) + self.padding] = x\n",
    "        return padded\n",
    "\n",
    "    def backward(self, x):    \n",
    "        y = x[:, :, self.padding:-self.padding, self.padding:-self.padding]\n",
    "        return y\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "class NNUpsample(Module):\n",
    "    def __init__(self, scale_factor):\n",
    "        super(NNUpsample).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input_size = x.size()\n",
    "        self.batch_size = x.size(0)\n",
    "        self.channels = x.size(1)\n",
    "        r = x.repeat_interleave(self.scale_factor, dim=2).transpose(2, 3).repeat_interleave(self.scale_factor, dim=2).transpose(2, 3)\n",
    "        return r\n",
    "\n",
    "    def backward(self, r):\n",
    "        \n",
    "        scale = self.scale_factor\n",
    "        res = []\n",
    "        for c in range(r.shape[1]): # aggregrate by channel\n",
    "            w = torch.zeros((self.batch_size, r.shape[1], scale, scale), device = device).float()\n",
    "            w[:, c, :, :] = 1\n",
    "            unfolded = unfold(r.float(), kernel_size=(scale, scale), stride=scale).float()\n",
    "\n",
    "            lhs = w.view(self.batch_size, r.shape[1],-1)\n",
    "            unfolded = unfolded.view(self.batch_size, scale**2,-1)\n",
    "            #print(\"unfolded.shape\",unfolded.shape)\n",
    "\n",
    "            #print(\"lhs.shape\", lhs.shape)\n",
    "            out_unf = lhs @ unfolded\n",
    "            \n",
    "            #out_unf = unfolded.transpose(1, 2).matmul(w.view(r.shape[1], -1).t()).transpose(1, 2)\n",
    "            #print(\"out_unf[:, 0, :].reshape((self.batch_size,r.shape[1],self.input_size[2], self.input_size[3]))\",out_unf[:, 0, :].reshape((self.batch_size,r.shape[1],self.input_size[2], self.input_size[3])).sum(1, keepdim=True).shape)\n",
    "            res.append(out_unf[:, c, :].reshape((self.batch_size,r.shape[1],self.input_size[2], self.input_size[3])).sum(1, keepdim = True))\n",
    "        res = torch.stack(res, dim = 1).sum(2)\n",
    "        return res\n",
    "\n",
    "    \n",
    "class Conv2d():\n",
    "    def __init__(self, out_channel, in_channel, kernel_size,stride = 1, padding = 0):\n",
    "        # Batch size?\n",
    "        self.kernel_size = kernel_size\n",
    "        self.out_channel = out_channel\n",
    "        self.stride = stride\n",
    "        self.input_shape = None\n",
    "        self.in_channel = in_channel\n",
    "        self.padding_ = padding\n",
    "        \n",
    "    def initialize(self, input):\n",
    "        input_height, input_width = input.size()[2:]\n",
    "        self.input_shape = (input_height, input_width)\n",
    "        self.batch_size = input.size(0)\n",
    "        self.output_shape = (self.out_channel, self.out_size(input_height, self.kernel_size, self.stride), self.out_size(input_width, self.kernel_size, self.stride))\n",
    "        self.weight_shape = (self.out_channel, self.in_channel, self.kernel_size, self.kernel_size)\n",
    "        self.weight = torch.empty(self.weight_shape,device=device).normal_()\n",
    "        self.bias = torch.empty(self.out_channel,device =device).normal_()\n",
    "\n",
    "    \n",
    "    def padding(self, input_size, ks, stride):\n",
    "        j = None\n",
    "        for i in range(10):\n",
    "            k = (input_size - ks + i) / stride\n",
    "            if k.is_integer() == True:\n",
    "                j = i\n",
    "                break\n",
    "        return j\n",
    "    \n",
    "    def out_size(self, s_in, ks, st):\n",
    "        return int((( s_in - (ks - 1) - 1 ) / st + 1) // 1)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if self.input_shape == None:\n",
    "            self.initialize(input)\n",
    "        input = input.float()\n",
    "        #self.padding_ = padding(self.input_shape[0], self.kernel_size, self.stride)\n",
    "\n",
    "        self.input = input\n",
    "        print(\"self.output_shape\",self.output_shape)\n",
    "        \n",
    "        unfolded = unfold(self.input, kernel_size= (self.kernel_size,self.kernel_size), stride = self.stride).float() \n",
    "        self.output = self.weight.view(self.out_channel, -1).float()  @ unfolded.float()  + self.bias.view(1,-1,1).float() \n",
    "\n",
    "        self.output = self.output.view(input.size(0),self.out_channel,self.output_shape[1] + 2*self.padding_, self.output_shape[2] + 2*self.padding_).float() \n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient):\n",
    "        print(\"output_gradient.size()\",output_gradient.size())\n",
    "        learning_rate = 0.01\n",
    "        \n",
    "        ## define some vars that I might delete\n",
    "        x, y = output_gradient.size()[-2:]\n",
    "        ks = self.weight_shape[-1] - 1\n",
    "        self.output_gradient = output_gradient\n",
    "\n",
    "        \n",
    "        self.weight_grads = torch.empty(self.weight_shape,device=device).zero_()\n",
    "        self.bias_grads = torch.empty(self.out_channel,device=device).zero_()\n",
    "        \n",
    "        \n",
    "        ### CALCULATE dL/dK\n",
    "        st_pad = self.padding(self.input.size(2), self.output_shape[1], self.stride) \n",
    "        print(self.padding(self.input.size(2), self.output_shape[1], self.stride))\n",
    "        print(st_pad)\n",
    "        st_pad2 = self.padding(self.input.size(3), self.output_shape[2], self.stride)\n",
    "        \n",
    "        zeros = torch.empty(self.input.size(0),self.input.size(1), self.input.size(2) + st_pad, self.input.size(3) + st_pad2, device = device).zero_().float()\n",
    "        print(\"input.size()\",self.input.size())\n",
    "        print(\"zeros.size()\",zeros.size())\n",
    "        \n",
    "        zeros[:,:,:self.input.size(2),:self.input.size(3)] = self.input[:,:,:self.input.size(2), : self.input.size(3)]\n",
    "        \n",
    " \n",
    "        self.input2 = zeros\n",
    "        \n",
    "        unfolded = unfold(self.input2.view(self.in_channel,self.input.size(0),  self.input2.size(2), self.input2.size(3)), kernel_size = self.output_shape[1:], dilation = self.stride, padding = 0, stride =self.stride)\n",
    "\n",
    "        print(\"output_gradient.view(self.out_channel,-1).size()\",output_gradient.view(self.out_channel,-1).size())\n",
    "        print(\"unfolded.size()\",unfolded.size())\n",
    "\n",
    "        wxb = output_gradient.view(self.out_channel,-1) @ unfolded#.view(self.in_channel,self.out_channel, -1)\n",
    "        print(\"wxb.size()\", wxb.size())\n",
    "        \n",
    "        actual = wxb.view(self.out_channel, self.in_channel,self.kernel_size, self.kernel_size).float()\n",
    "        self.weight_grads += actual\n",
    "                \n",
    "        size_grad = self.output_gradient.size()[-2:]       \n",
    "        \"\"\"\n",
    "        ### CALCULATE dL/dX\n",
    "        (bs, ic, h, w) = self.input.shape\n",
    "        (oc,ic,s0,s1) = self.weight.shape\n",
    "        (bs,oc,oh,ow) = self.output_gradient.shape\n",
    "        blocks = (self.output_gradient.view(bs,oc,oh*ow).transpose(1,2).reshape(-1,oc).float().mm(self.weight.view(oc,-1).float())).reshape(bs,oh*ow,-1).transpose(1,2)\n",
    "        \n",
    "        return fold(blocks,(h,w), (s0,s1),stride= self.stride, padding = self.padding_).reshape(self.input.shape)\n",
    "    \n",
    "        \"\"\"\n",
    "        # we will flip the kernel to do the full convolution\n",
    "        self.kernel_flipped = self.weight.flip([2,3])\n",
    "        \n",
    "        # unstride the output gradient    \n",
    "        \n",
    "\n",
    "        zeros = torch.empty(self.input.size(0),self.out_channel,(x-1)* (self.stride-1)+x + st_pad , y + (y-1)* (self.stride -1) + st_pad2).zero_()\n",
    "        zeros[:,:,::self.stride,::self.stride] = output_gradient\n",
    "        \n",
    "        self.unstrided_gradient = zeros        \n",
    "        \n",
    "        \n",
    "        unfolded = unfold(self.unstrided_gradient, kernel_size= self.kernel_size, stride = 1, padding = (self.kernel_size - 1, self.kernel_size - 1))\n",
    "        \n",
    "        lhs = self.kernel_flipped.view(self.in_channel, self.kernel_size ** 2 * self.out_channel)\n",
    "        #lhs = self.kernel_flipped.view(self.in_channel, -1)\n",
    "\n",
    "        self.input_grad = lhs @ unfolded\n",
    "                \n",
    "        self.input_grad = self.input_grad.view(self.input.size(0),self.in_channel,self.input_shape[0], self.input_shape[1])     \n",
    "\n",
    "        # CALCULATE dL/dB\n",
    "        \n",
    "        self.bias_grads += self.output_gradient.mean((0,2,3))\n",
    "\n",
    "        self.weight -= learning_rate * self.weight_grads\n",
    "        self.bias -= learning_rate * self.bias_grads\n",
    "\n",
    "        return self.input_grad\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "class Sigmoid(Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Sigmoid).__init__()\n",
    "\t\n",
    "\tdef __call__(self,x):\n",
    "\t\treturn self.forward(x)\n",
    "\t\n",
    "\tdef forward(self,x):\n",
    "\t\treturn 1 / ( 1 + torch.exp(-x) )\n",
    "\t\n",
    "\tdef _grad(self, x):\n",
    "\t\treturn self.forward(x) * (1-self.forward(x))\n",
    "\t\n",
    "\tdef backward(self, gradwrtoutput):\n",
    "\t\treturn self._grad(gradwrtoutput)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "################### FUNC ###################\n",
    "\n",
    "\n",
    "def train(model, train_input, train_label, criterion, nb_epochs,\n",
    "\t\t\tmini_batch_size, eta, loss_flag=0, verbose=1):\n",
    "\t\"\"\"\n",
    "\t\targs:\n",
    "\t\t\tmodel: MLP we want to train\n",
    "\t\t\ttrain_input: inputs of coordinates Nx2\n",
    "\t\t\ttrain_label: inputs for ground truth, points belong to circle or not (1 for belonging)\n",
    "\t\t\tcriterion: MSE\n",
    "\t\t\tnb_epochs: number of epochs\n",
    "\t\t\tmini_batch_size: number of datapoints used for one weight update\n",
    "\t\t\teta: learning rate for weight update\n",
    "\t\t\tloss_flag: set to 0 when using MSE and set to 1 when using CROSSENTROPY\n",
    "\t\t\tverbose: flag to print logs during training \n",
    "\t\t\t\n",
    "\t\texample:\n",
    "\t\t\tmodel = Net(hidden=25, activation='tanh')\n",
    "\t\t\tlosses, accs = train(model, ...)\n",
    "\t\"\"\"\n",
    "\n",
    "\tlosses = []\n",
    "\taccs = []\n",
    "\tfor e in range(nb_epochs):\n",
    "\n",
    "\t\tfull_loss = 0\n",
    "\n",
    "\t\tfor b in range(0, train_input.size(1), mini_batch_size):\n",
    "\t\t\ttrain_batch = train_input[:,b:b+mini_batch_size]\n",
    "\n",
    "\t\t\toutput = model.forward(train_batch) # (2,batch_size)\n",
    "\t\t\tground_truth = train_label[:,b:b+mini_batch_size] # (2,batch_size)\n",
    "\t\t\tif loss_flag:\n",
    "\t\t\t\tground_truth = torch.argmax(ground_truth,axis=1)\n",
    "\n",
    "\t\t\t# reset gradients before propagating\n",
    "\t\t\t#model.reset_gradients()\n",
    "\n",
    "\t\t\t# compute loss & # backprop the loss\n",
    "\t\t\tloss = criterion.forward(ground_truth, output) \n",
    "\t\t\tfull_loss += loss\n",
    "\t\t\tmodel.backward(criterion.backward(ground_truth, output))\n",
    "\n",
    "\n",
    "\t\t\t# optim \n",
    "\t\t\tfor i in range(len(model.full.module)):\n",
    "\t\t\t\tlayer = model.full.module[i]\n",
    "\t\t\t\tif not len(layer.param()): \n",
    "\t\t\t\t\t# case of activation, no need to update any params\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t#layer.weight -= eta * layer.dldw\n",
    "\t\t\t\t#layer.bias -= eta * layer.dldb \n",
    "\n",
    "\n",
    "\t\tif  (e % 10 == 0): \n",
    "\t\t\t# eval\n",
    "\t\t\toutput = model.forward(train_input,eval_mode=1)\n",
    "\t\t\tacc = accuracy(output, train_label)\n",
    "\t\t\tif verbose:\n",
    "\t\t\t\tprint('Epoch {}: train accuracy-> {} | train loss-> {}'.format(e,acc,full_loss/train_input.size(1)))\n",
    "\n",
    "\t\t\tlosses.append(full_loss/train_input.size(1))\n",
    "\t\t\taccs.append(acc)\n",
    "\n",
    "\treturn losses, accs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1324,
   "id": "b4b18988",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "mse=MSE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1325,
   "id": "4b4e122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "usample = NNUpsample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1335,
   "id": "ce2481a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((1,5,32,32), device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1336,
   "id": "d36356c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usample.backward(usample.forward(x)).sum(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1337,
   "id": "176ed9d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1337]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mSequence\u001b[49m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequence' is not defined"
     ]
    }
   ],
   "source": [
    "Sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1338,
   "id": "a5e2f23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.output_shape (48, 16, 16)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (256x20 and 12x48)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1338]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [1332]\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [1332]\u001b[0m, in \u001b[0;36mSequential.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 27\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [1332]\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     31\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m transf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules:\n\u001b[0;32m---> 32\u001b[0m \t\tx \u001b[38;5;241m=\u001b[39m \u001b[43mtransf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "Input \u001b[0;32mIn [1332]\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself.output_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_shape)\n\u001b[1;32m    265\u001b[0m unfolded \u001b[38;5;241m=\u001b[39m unfold(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput, kernel_size\u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size), stride \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_)\u001b[38;5;241m.\u001b[39mfloat() \n\u001b[0;32m--> 266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_channel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43munfolded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat() \n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m),\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channel,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_shape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_shape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_)\u001b[38;5;241m.\u001b[39mfloat() \n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (256x20 and 12x48)"
     ]
    }
   ],
   "source": [
    "model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1097,
   "id": "9ad48363",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Conv2d2():\n",
    "    def __init__(self, out_channel, in_channel, kernel_size,stride = 1, padding = 0):\n",
    "        # Batch size?\n",
    "        self.kernel_size = kernel_size\n",
    "        self.out_channel = out_channel\n",
    "        self.stride = stride\n",
    "        self.input_shape = None\n",
    "        self.in_channel = in_channel\n",
    "        self.padding_ = padding\n",
    "        \n",
    "\n",
    "    \n",
    "    def set_initial(self, weight, bias):\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "\n",
    "    def initialize(self, input):\n",
    "        input_height, input_width = input.size()[2:]\n",
    "        self.input_shape = (input_height, input_width)\n",
    "        self.batch_size = input.size(0)\n",
    "        self.output_shape = (self.out_channel, self.out_size(input_height, self.kernel_size, self.stride), self.out_size(input_width, self.kernel_size, self.stride))\n",
    "        self.weight_shape = (self.out_channel, self.in_channel, self.kernel_size, self.kernel_size)\n",
    "        self.weight = torch.empty(self.weight_shape,device=device).normal_()\n",
    "        self.bias = torch.empty(self.out_channel,device =device).normal_()\n",
    "\n",
    "    \n",
    "    def padding(self,input_size, ks, stride):\n",
    "        if stride == 1:\n",
    "            return 0\n",
    "        else:\n",
    "            quotient = math.ceil(input_size / stride)\n",
    "            a =   (((input_size - stride+1) % ks)) % stride\n",
    "        return a\n",
    "\n",
    "    \n",
    "    def out_size(self, s_in, ks, st):\n",
    "        return int((( s_in - (ks - 1) - 1 ) / st + 1) // 1)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if self.input_shape == None:\n",
    "            self.initialize(input)\n",
    "        input = input.float()\n",
    "\n",
    "        self.input = input\n",
    "        print(\"self.output_shape\",self.output_shape)\n",
    "        unfolded = unfold(self.input, kernel_size= (self.kernel_size,self.kernel_size), stride = self.stride, padding = self.padding_).float() \n",
    "        self.output = self.weight.view(self.out_channel, -1).float()  @ unfolded.float()  + self.bias.view(1,-1,1).float() \n",
    "\n",
    "        self.output = self.output.view(input.size(0),self.out_channel,self.output_shape[1] + 2*self.padding_, self.output_shape[2] + 2*self.padding_).float() \n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient):\n",
    "        \n",
    "        learning_rate = 0.01\n",
    "        \n",
    "        ## define some vars that I might delete\n",
    "        x, y = output_gradient.size()[-2:]\n",
    "        ks = self.weight_shape[-1] - 1\n",
    "        self.output_gradient = output_gradient\n",
    "\n",
    "        \n",
    "        self.weight_grads = torch.empty(self.weight_shape,device=device).zero_()\n",
    "        self.bias_grads = torch.empty(self.out_channel,device=device).zero_()\n",
    "        \n",
    "        \n",
    "        ### CALCULATE dL/dK\n",
    "\n",
    "        zeros = torch.empty(self.input.size(0),self.input.size(1), self.input.size(2) - self.padding(self.input.size(2), self.output_shape[1], self.stride), self.input.size(3) - self.padding(self.input.size(3), self.output_shape[2], self.stride), device = device).zero_().float()\n",
    "        zeros[:,:,:self.input.size(2),:self.input.size(3)] = self.input[:,:,:self.input.size(2) - self.padding(self.input.size(2), self.output_shape[1], self.stride), : self.input.size(3) -self.padding(self.input.size(3), self.output_shape[2], self.stride)]\n",
    "        \n",
    " \n",
    "        self.input2 = zeros\n",
    "        \n",
    "        unfolded = unfold(self.input2.view(self.in_channel,self.input.size(0),  self.input2.size(2), self.input2.size(3)), kernel_size = self.output_shape[1:], dilation = self.stride, padding = self.padding_, stride = 1)\n",
    "\n",
    "        print(\"output_gradient.view(self.out_channel,-1).size()\",output_gradient.view(self.out_channel,-1).size())\n",
    "        print(\"unfolded.size()\",unfolded.size())\n",
    "\n",
    "        wxb = output_gradient.view(self.out_channel,-1) @ unfolded#.view(self.in_channel,self.out_channel, -1)\n",
    "        print(\"wxb.size()\", wxb.size())\n",
    "        \n",
    "        actual = wxb.view(self.out_channel, self.in_channel,self.kernel_size, self.kernel_size).float()\n",
    "        self.weight_grads += actual\n",
    "\n",
    "                \n",
    "        size_grad = self.output_gradient.size()[-2:]       \n",
    "        \n",
    "        ### CALCULATE dL/dX\n",
    "\n",
    "        # we will flip the kernel to do the full convolution\n",
    "        self.kernel_flipped = self.weight.flip([2,3])\n",
    "        \n",
    "        # unstride the output gradient        \n",
    "        zeros = torch.empty(self.input.size(0),self.out_channel,(x-1)* (self.stride-1)+x + self.padding(self.input.size(2), self.output_shape[1], self.stride) , y + (y-1)* (self.stride -1) + self.padding(self.input.size(2), self.output_shape[1], self.stride)).zero_()\n",
    "        zeros[:,:,::self.stride,::self.stride] = output_gradient\n",
    "        \n",
    "        self.unstrided_gradient = zeros        \n",
    "        \n",
    "        \n",
    "        unfolded = unfold(self.unstrided_gradient, kernel_size= self.kernel_size, stride = 1, padding = (self.kernel_size - 1, self.kernel_size - 1))\n",
    "        \n",
    "        lhs = self.kernel_flipped.view(self.in_channel, self.kernel_size ** 2 * self.out_channel)\n",
    "        #lhs = self.kernel_flipped.view(self.in_channel, -1)\n",
    "\n",
    "        self.input_grad = lhs @ unfolded\n",
    "                \n",
    "        self.input_grad = self.input_grad.view(self.input.size(0),self.in_channel,self.input_shape[0], self.input_shape[1])     \n",
    "\n",
    "        # CALCULATE dL/dB\n",
    "        \n",
    "        self.bias_grads += self.output_gradient.mean((0,2,3))\n",
    "\n",
    "        self.weight -= learning_rate * self.weight_grads\n",
    "        self.bias -= learning_rate * self.bias_grads\n",
    "\n",
    "        return self.input_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd3d1e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68492f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714f39d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
